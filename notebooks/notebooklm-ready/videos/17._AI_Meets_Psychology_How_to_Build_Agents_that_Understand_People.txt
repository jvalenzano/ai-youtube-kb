# 17. AI Meets Psychology: How to Build Agents that Understand People

**Video ID:** RYiWt_u2h6Y
**Channel:** SiliconANGLE theCUBE
**Duration:** 45:17
**URL:** https://www.youtube.com/watch?v=RYiWt_u2h6Y

**Module:** Foundations of AI Agents
**Module Rationale:** This video focuses on fundamental limitations of current LLMs and introduces psychological intelligence as a foundational capability needed for effective AI agents, rather than specific workflows or implementations

## Summary
- LLMs understand language syntax and semantics but lack psychological intelligence to understand how humans think, feel, and communicate
- Receptiviti has developed psychological AI that analyzes function words (pronouns, prepositions) that traditional NLP ignores but carry psychological signals
- 78% of business professionals agree psychological understanding is critical for AI to work effectively with humans
- Current LLMs treat all users the same way, missing opportunities to personalize responses based on psychological profiles
- Psychological AI can detect traits like analytical thinking, stress levels, depression indicators, and personality dimensions from language patterns
- Trust is the key success factor for generative AI adoption - psychological awareness builds trust through empathetic interactions
- The technology works by converting language into 200+ psychological vectors that can be fed into LLMs to personalize responses
- Extended models combining LLMs with psychological intelligence, causal reasoning, and semantic knowledge represent the next frontier of AI
- Real-time psychological profiling enables dynamic adaptation to users' current emotional and cognitive states during conversations

## Key Takeaways
- **DO:** Integrate psychological intelligence APIs with LLMs to personalize agent responses based on user personality and emotional state
- **DO:** Analyze function words and linguistic patterns that traditional NLP ignores to understand user psychology
- **DO:** Build psychological profiles dynamically from conversation history to adapt agent behavior in real-time
- **DON'T:** Rely solely on LLMs for human interaction - they lack the psychological depth needed for trustworthy collaboration
- **DON'T:** Ignore the psychological context when designing AI agents that need to establish trust with human users

## Topics
psychological AI, agent personalization, LLM limitations, trust in AI, human-AI collaboration, psychological profiling, agentic workflows

## Key Moments
- [07:12](https://www.youtube.com/watch?v=RYiWt_u2h6Y&t=432s): Explains the 'illusion of understanding' - LLMs sound human but lack psychological depth
- [12:19](https://www.youtube.com/watch?v=RYiWt_u2h6Y&t=739s): Discusses MIT survey showing 78% of professionals want psychologically aware AI
- [25:38](https://www.youtube.com/watch?v=RYiWt_u2h6Y&t=1538s): Explains how function words carry psychological signals that LLMs ignore
- [32:51](https://www.youtube.com/watch?v=RYiWt_u2h6Y&t=1971s): Demonstrates password reset example showing generic vs psychologically-aware responses
- [40:01](https://www.youtube.com/watch?v=RYiWt_u2h6Y&t=2401s): Presents framework for extended models combining multiple AI capabilities beyond LLMs

## TL;DR
AI agents need psychological intelligence beyond LLMs to understand and empathetically collaborate with humans.

## Full Transcript

>> Hello everyone.
Welcome back to the Next
Frontiers of AI podcast where we speak with the pioneers who are not only shaping the future of AI, but helping bring the value of AI to life.
I'm Scott Hebner, Principal Analyst for AI at SiliconANGLE
Media and theCUBE Research and thanks so much for joining us.
We're entering a new era
of intelligent agents that can reason, act and collaborate across business workflows, but there are limitations to what agents can do when
built solely on LLMs, which compromises their
ability to be trusted in real- world business environments.
In past episodes, we've talked about how data is not knowledge, how predictions are not judgments, and how reasoning requires causality, but there's yet another major
limitation holding agents back from becoming effective and
trustworthy digital coworkers.
While LLMs may be fluent in
language, they can't grasp

[01:00]
how humans think, feel,
rationalize, or act.
Simply said, they understand
language, but not people.
That's a very big difference.
They're missing the
psychological context that shape how people communicate,
establish goals, make plans, evaluate situations, make
decisions, and so on and so forth.
In a world where agents must engage and collaborate, not just
respond, this illusion of understanding becomes
a critical risk factor, especially in high-stake business and domains and environments.
So what if we can change all that?
What if AI could infer
cognitive state, emotional tone, and a mindset from everyday language and use it to become more
psychologically intelligent collaborators with human beings and it's making them human aware?
Today's guest is an innovator
in this exciting new field of AI, and I'm excited to
welcome Jonathan Kreindler, the President and
Co-Founder of Receptiviti.

[02:01]
They have spent the last
decade advancing the science of language-based psychological insight.
Jonathan and his team are now
pioneering a new layer of AI, one that allows agents to
understand who they're interacting with, not just what was said.
Jonathan, welcome to the podcast.
It is great that you have
taken the time to join us.
I'm really excited about this
conversation as I talk to more and more of you pioneers out
there and finding new angles and new things that are
really shaping the future, and this one makes a ton of sense to me.
So thanks for being here.
>> Thanks, Scott. Thanks for having me.
>> Let's start with just a
little bit of background for our audience.
Can you just tell us a little
bit about your personal journey and what led you to where you are now leading Receptiviti?
>> Sure. After doing an undergrad degree in psychology and a master's
degree in business, I had about a 15-year run as a
management consultant working for large firms and
smaller boutique firms.

[03:05]
And in around 2009, I started my own practice, my own firm.
And we were doing work with everybody from government entities to
Fortune 100s to start-ups.
And one of my clients was SAP for years, no NDA there.
And around 2013, I was doing work with one of
the folks who was responsible for awareness marketing, and he was explaining to me
that they were struggling to really build messaging that resonated with their target audience.
And this was a gentleman
who was responsible for awareness marketing at SAP.
And he was expressing that part of that struggle was they
invested heavily in an agency to help them refine these personas and really, really finesse
the messaging in a way that would resonate with the audience.
And he asked me if I might
be able to help with that, if my team could help,

[04:05]
and we thought we could
probably figure something out.
And I started where so many
of us do when we're trying to solve a problem that we
haven't encountered directly before, started digging into the research.
And I happened to stumble upon
some really interesting work that was not necessarily
done to solve that problem, but I very quickly
realized could be adapted to solve the problem that
that client was facing.
And it was the work of a
gentleman named James Pennebaker, professor James W.
Pennebaker at the University
of Texas at Austin.
And make a long story short,
Jamie, as we call him, he became one of the
co-founders of Receptiviti.
The two of us started it back in 2015.
What Jamie discovered was that language provides a tremendous amount
of signal into the way that people feel, into their
psychological state, into their cognitive processes.
And while that might
seem somewhat obvious, what isn't obvious is that traditional natural language
processing techniques like

[05:07]
those used by just about everybody, including large language models, don't pick up on those signals because they ignore a lot of the language that carries most of that signal.
We can get into that in a bit.
But basically Jamie, through his research, made this discovery and started building this body of research and science that delved into the way that people use these
stock words, these words that are removed by traditional NLP and all of the signal
that comes out of it.
His body of research is immense.
There's now 30, 000 citations on Google
Scholar of his science.
And we decided once we realized we could fix the problem or help this client solve
their marketing problem, we started realizing there were more and more applications
for this sort of science.
So we established Receptiviti
as the commercial side to what Jamie had been doing as an academic and began this really interesting
journey of a great deal of consulting work in the early days.

[06:09]
And that evolved into some more solution focused
applications that we built.
But as we fast-forward to today, in 2021, we launched an API.
It really forms the
foundation of our business.
And our customers are
companies that really want to better understand
the psychology of people that matter to their business.
And so they'll wire their
technology into our API, they'll run language through it from those that they want to better understand.
And the system generates
about 200 different signals of psychology, cognitive processes and other factors that
sit below the surface, beyond the obvious of what
people are talking about.
This really gets into who they are and where they're at at
that point in time from a psychological perspective.
>> So as we've sort of now
entered into this era of AI, what are you seeing in the
enterprise, your customer set, and as you're gauging
out there with companies and particularly those
that are, I think most of them are starting to
transition to, it's not just about applying AI, that it's
got to be trustworthy AI

[07:12]
as they expect it to do more and more than just be a tool
for analyzing information or generating documents or whatever.
And you've been talking a lot
out there about the illusion of understanding.
>> Yeah.
- Can you just explain that a little bit and how that's related to
what people are using today with the LLMs?
>> Yeah, that's a really good question.
So much focus on large language models and their capabilities and
how they're being integrated or these bots and call center systems are
now leveraging large language models to act on behalf of
organizations, to be that voice, to be that interface.
And they're trained on huge data sets, and we've seen it over the past couple of years now since the public launch of large language models, we've seen how good they have become
at communicating with us.
You can get onto ChatGPT, you
can spend a whole day chatting with ChatGPT about anything you want.
And if you put somebody
in front of ChatGPT

[08:12]
who had no familiarity with
what they were interacting with, they would probably think there
was another person sitting at the other end who was just really, really fast at doing everything.
We've gotten to a point where we've got large language models and
these systems that sound human, they almost feel human.
They don't quite feel human,
but they get pretty darn close.
And they're really, really
good at syntax and semantics and they put everything in front of us and it looks like coherent language and it is coherent language, but it's about this deep at
the end of the day, right?
There's not a lot below the surface there.
And I don't mean if you ask a
large language model a follow- along question, it's not
going to be able to answer it.
It is. It's just it doesn't
really know much more than what it is reading, the surface level of the data it's trained on and
what it's presenting to you.
And so when you're interacting
with a large language model, you can ask it a question,
it'll give you a fact- based response, but it can't really tell
anything about you based on

[09:14]
what you're asking beyond
the fact that you don't know what it is you're asking about, right?
It can't make inferences
about your confidence level or how analytical you are or other factors that could go a long way at
informing the way it responds back to you with a more tailored response based on who you are.
So if I were a very
highly analytical person and I asked a large
language model a question, it might not respond to me
in that highly analytical way that would really jive with who I am and actually do a better job
of presenting me information with a way that I would best receive it or I would want to receive it.
And so what we're seeing
today are these large language models that are being used
as the backbone of these agentic systems and these
bot-based interfaces and call centers and whatnot that are superficially intelligent but not psychologically
deep or intelligent.
And so what's interesting
is they're interacting with people routinely.
This is what they do. But as much

[10:15]
as they understand the subject matter of what they're presenting, maybe not understand isn't the right word, but they can regurgitate
the subject matter that they're presenting,
they really know very, very little about the people
they're interacting with.
And as these are systems that,
these large language models and these AI systems, are
really technology systems built by humans for humans, there's
a bit of a disconnect here between the audience they're intended to serve and the psychological depth and understanding that they
need to really interact with people in the best way possible.
You and I are talking right
now, right now I'm doing most of the talking, but in
conversations we've had offline, there's social cues we pick up on and nuances in the way
that we communicate.
>> Right.
- I read you, you read me, we understand, you see a look on my face, you can tell them I'm
not quite understanding what you're saying or vice versa.
These are things that are
completely missed by technology and when we interact with technology.
And a lot of those signals
are actually conveyed

[11:17]
in the text itself.
So when the systems are interpreting the text
at the surface level, they miss these social psychological cues and a lot of that subtext.
And so what our science has done is it's kind of bringing the psychological subtext above the surface now so that these large language
models can begin to understand, "Okay, the person is talking about XYZ and they're talking about
it with a great deal of confidence," or, "They're
talking about it with a lot of cognitive load, maybe
they don't quite understand what they're talking about as much.
" And so it really rounds
out the understanding of these large language
models so they can communicate with a lot more acuity, in
a way that is much more...
it drives better with the
audience at the end of the day.
That's just one aspect of it.
>> Yeah, I would imagine that
becomes increasingly important.
It's one thing where
generative AI has been used as a tool for people.
>> Yeah. - As it starts to
move into being a true partner that's supposed to be,
like in the work world,

[12:19]
they're being paired up with
human beings to get work done and they've got to establish
goals and plan something and make decisions together
and they're there to support.
I would imagine it's one
thing to get information and ask questions, like
you said, when we talk, get responses, it's another
whole thing if you're trying to establish a goal.
I would imagine the psychological thing becomes even more important.
And actually, let me show you a survey that I actually found out there, because when we talked, it really intrigued me, this whole area.
So I went out and I did some research and there's actually a lot out there.
There's some stuff out
there from McKinsey.
I saw some from the
Boston Consulting Group.
This one happens to be from
MIT, and it's not that old.
I just think a couple
months ago they published this study that they did.
And you can see here, 78% of
business professionals agreed that the psychological and
emotional understanding is really critical for AI to
successfully work with humans.
>> Yeah.
- And there was also a data point in there that said that 62%

[13:21]
of business executives would
feel a lot more comfortable accelerating the deployment
of digital workers or digital labor if they knew
that the agents were able to work more effectively with their human counterparts, which comes to this in many ways, right?
>> Yeah.
- Basically my takeaway from reading everything was this is
a need in businesses and 78% is a pretty big percentage, right?
>> Yeah.
- So it's not some foreign cool idea.
This is real stuff that
people are thinking.
>> Yeah, yeah. Look, I
think we're depending more and more on these systems
at the end of the day, and sometimes we don't even know that we're depending on
them at the end of the day.
I was dealing with some
travel challenges a couple of weeks ago and put a call into...
I think it was Expedia.
And you end up with this bot who's answering your questions.

[14:21]
And I've got a bias here because the second that I
start interacting with the bot, I'm testing it a little bit and trying to understand how much of me does it really understand,
and it knows what I'm saying, but can it get me?
And I think this is where we're going to start seeing really, really
big changes over the next few years, where not bots necessarily that, it's not about fooling humans
into believing the bots are real, because that's probably something
we shouldn't be doing.
I want to know I'm chatting with a bot.
If it can really understand me, it can do a better job of helping me.
But the other side of
it too is the empathy part of it too, right?
If the bot is completely
flat and I'm getting more and more agitated or frustrated because this process is
taking a really long time to resolve whatever my inquiry or my problem is, it would
be really, really nice if the bot could actually
acknowledge that, recognize it.
It doesn't even need
to say anything to me.

[15:23]
But if it could change its
course a little bit, recognizing that my tone is changing, it provides a feeling of a little
bit more understanding.
And it sounds like a really strange thing to say when you're
interacting with something that isn't real, but humans
are really real that way.
We look for feedback in places
sometimes that we shouldn't.
And we want systems to treat us as humans, even though when they're not, right?
>> Yeah, I could imagine if
you're working, you have a goal with an agent and you're trying to make some decisions on the
best way to achieve that goal.
And if you're not really
feeling good about the recommendation and you're hesitant or stressed about it, it would be really cool if
it recognized that some way and then says, "Hey, want me to give you some more proof points?
" Whatever. I get this.
And one of the things I've
definitely, in the research that we've been doing at theCUBE Research, is the overriding success factor

[16:24]
for generative AI is going
to be the trust factor.
>> Yeah.
- Not in the sense necessarily of security and compliance and data
privacy and all that, but in the sense that if
human beings don't trust their digital co-worker, they're
not going to use it.
They're not going to rely on it.
It's not going to become a true partner.
>> Yeah. - And I think this goes
a long way as it matures to helping people trust what they're doing with the agent, right?
>> Yeah. I saw some research recently, and I am not going to be
able to quote the numbers or the source unfortunately, but it was speaking about how the desire for AI systems that truly understand
us at the end of the day and people's willingness to
interact with these systems that much more when they feel
like the system actually gets who they are and speaks to
them as a person as opposed to just being kind of the rote
same thing person to person.
So yeah, I think as humans, we want that.
I think from a business perspective,

[17:25]
we're seeing this proliferation of AI bots now being
used across industries.
This is where there's so many companies now
that are being funded.
These are kind of front ends
to back ends, which are powered by the LLM companies
that we're all aware of.
But those systems are limited
in many ways, in a lot of the ways that we just discussed.
All of these companies are
going to need differentiators at the end of the day.
And at the early stages, that intuitive system that really
understands humans will be a huge differentiator.
But I don't think we're
very long from a point where it's table stakes where
systems just need to be able to understand us at that level.
And I think we're talking
about systems that are, what I've been talking
about in this conversation, has been more customer
support sort of systems, but think about when we
get to digital health, mental health sort of support, and you've got systems that right now,

[18:27]
they're not that intelligent from a psychological perspective.
Again, some of them, early
days, they were scripted.
Now, they're less scripted
because of large language models and the generative component, but they still don't have that depth of psychological understanding.
We're not too far away from a point where we've got large language
model based generative AI that is...
or, I shouldn't say that,
sorry. Generative based psychological support systems that can really start picking
up on some of the signals of various diagnoses and understand that people
who are showing signs of depression use language
this way much more so than people who aren't.
I think at the end of the
day, there's no question that there's a huge gap to be filled here.
We're never going to get
to a point where these systems are as...
there's all this talk about
artificial general intelligence.
We're we're probably a decade
plus away from that at the end of the day for a whole
bunch of different reasons.
What is intelligence and what does intelligence mean

[19:27]
to me versus a different culture?
They're very different definitions, but if we're going to get
anywhere near artificial general intelligence, we're
going to need that kind of social psychological
aspect, that kind of fluency and understanding people that these systems
simply don't have today.
>> Yeah, again, my view of
this, the very practical view of it is that, again, the ability to innovate, think of it
this way, the currency of innovation is becoming
trust, particularly in AI.
No trust, there's no ROI.
And when you're dealing with human beings that are interacting with AI, the more you can get those
human beings to trust what they're interacting with, and this makes, again, a
ton of sense on something that can help really create
that trust, if you're someone that's, I can just imagine, maybe it jokes with you a little bit if it
senses that you're trying to get something done, but if
you're really stressed out,

[20:30]
you don't do that.
All those different kinds
of things, I agree with you.
Whether you're calling a
customer support as a customer or whether you're an employee
doing work with an agent, it just makes a ton of sense.
>> Yeah. I think there's one
other piece though, which is the circumsparency piece, which
is it's tied to trust.
How do you ensure these systems
don't direct people down a path that they shouldn't direct them down or be overly positive in its responses to what a person is saying
even when it shouldn't be being positive?
When you can layer in more
psychological understanding, you can start understanding
when the system is being overly positive towards something that it probably shouldn't
be being overly positive.
You can stop it from
encouraging negative sorts of behavior at the end of the day.
You can start understanding
when, for example,

[21:33]
when the rapport has just kind of gone on too long at the end of the day, when there are other signals
there that should be suggesting that the large language model
in this situation back off a little bit and stop
encouraging a certain behavior.
>> Yeah, we can probably
go on all day with just what we think about this, even
the person who's interacting, what's their risk tolerance
on a recommendation?
>> Yeah. - Are they more
conservative and they want a lot of proof and maybe the model will want to make sure it's very high
confidence level things that are being provided
where maybe for someone else the use cases can go on forever here.
>> Absolutely. - So let's talk
about the LLMs for a minute because, again, I've been on
this mantra about LLMs alone.
To me, they're the gateway
into the world of AI.
They're democratizing it for
everybody around the world, but they're the equivalent of the browsers during the internet age.
It's how you build around
that that's really going to make a difference by
extending extended models that then interact with the LLMs

[22:35]
and through a generative AI interface.
So you have argued that you need more than just fluency, I mean, I know language, right?
It's not the same thing as understanding and that AI systems will fail
to truly engage with humans, and that's why this
psychological context is missing.
Talk to me about what you
see as missing in the LLMs and why they can't do that.
>> Yeah, these large
language model systems, I'm not saying anything here
that you don't already know, these large language model
systems have been developed to predict the next work
at the end of the day.
And they do that remarkably well.
So much so that you could
be fooled into thinking that you're talking to a
human in a lot of cases.
But being trained to predict the next word and have fluency in that way is a very, very different thing than being able to understand the meaning of words

[23:35]
or understand the implications
of certain words being used or rates and frequencies and repetition of words and whatnot.
And so really dealing
two different things.
On one side is the generative.
How have these models been built to generate language based on the language they've been trained on?
But flipping it around and
saying can these models actually infer what's happening beneath the surface based on
the language that is being used by the person that they're
interacting with is just something they weren't developed
to do in the first place.
And so what they're pretty
good at is understanding these kind of overt signals.
If somebody is expressing language that is typically involved with stress, "I can't believe this has happened," or, "I don't have time to get this done," or, "There's so much going
on," it'll pick up on that because quite logically,
those are stress-related sorts of utterances and phrases, right?
>> Yeah.
- But what they don't know how to do is they don't have the ability to

[24:37]
understand at a very, very deep level how people communicate differently
when they're stressed out and not showing that stress or not communicating that stress.
Or if somebody is depressed,
if somebody is not saying, "I feel depressed," or if
somebody is not saying, "I'm really not happy today," right?
Models will understand that.
>> Yeah, because everyone does this.
>> They get that, right?
But if I'm a person who is showing signs of depression
and I'm not being too overt but I'm using more
first-person singular pronouns and more absolutist language, the large language model
probably won't get that.
But a system like ours will
pick up on that as some of the very, very significant
indicators that are associated with people who are showing
signs of depression.
And so what you've got are
you've got these great systems that are incredibly,
incredibly good at mimicking and sounding like humans
without the depth.

[25:38]
They don't understand, right?
They're looking for
correlations, they're looking for relationships, but they
don't necessarily know how to read us as humans.
And another part of it too is when analyzing language, they're
not looking at prepositions and they're not looking at pronouns and function words as anything
other than just words, right?
>> Right.
- What we know is prepositions, pronouns, these function words, what are
sometimes called stop words, hold the lion's share of
psychological signal and language.
And traditional NLP
completely ignores them because the words that we're
interested in, he, she, they, words like because, although,
they have no context.
And so a traditional NLP will look at it and say, "Eh, don't need it.
No context. It doesn't
matter when it comes to topic and high level sentiment analysis," but these words are incredibly, incredibly important when you're trying to understand human psychology.
They're the social words
at the end of the day, the psychological words, the
words that help us understand

[26:40]
where people are and how
they relate to other people.
>> Yeah, because you are right.
The LLMs, they provide a
really critical foundation, and they're basically
statistical probability engines that crank through, they swim
in these huge data lakes, lakes of data, and they establish patterns and correlations, like
you said, next best token that then forms a word
and so on and so forth.
But I love the chart that we
had talked about and you had provided here, which kind of really, I think, nets it out, right?
They treat everyone
the same, except, well, maybe you can talk
through these four things?
>> Yeah, absolutely. If you and I were to speak to an
LLM or engage with an LLM and ask the same question, we might use slightly different
words to ask the question, but the question would be the same.
And the LLM would largely
treat us the same way, right?
A smarter system, a more psychologically
aware system would look at

[27:41]
how we are asking that question and recognize the difference
in how reflects where each of us is coming from
somewhere slightly different and address us or respond to us in a slightly
different way based on where we're coming from, even though the question is the same.
So yeah, they absolutely
do treat users the same.
More informed systems would not do that at the end of the day.
In an ideal world, they're going to personalize the experience.
Right now, they're responding
with language that is relevant to the task at hand, but they're not necessarily
personalizing the experience or changing the way they say
things based on who we are and how we communicate and really what our communication style
and psychological state is.
They understand a lot about
people, facts at the end of the day, but they don't
really understand more than superficial emotions and
the way that people think.

[28:41]
With the right sort of
analysis, you can tell that somebody is a
highly analytical thinker or is much lower on the analytical scale.
A large language model is
pretty much blind to that, is going to respond the
way it responds, right?
If I'm a highly analytical
individual, I'm going to appreciate facts and data
and points of comparison.
But again, LLMs don't
have that capacity yet.
And ultimately, as the
slide shows, they really do, they lack that human
psychological context.
And context is this word
that is being used more and more and more now.
How do I provide more context to the LLM to get a better response?
I think I've seen 10 different
articles just the past two days about context in
prompts and context in LLMs.
And one of the big pieces
that's missing at the end of the day, especially in
systems that were built by humans to interact with and support humans, is the human context at
the end of the day, that psychological context.
So I think we're getting
there, but we're a long way.

[29:43]
>> We're a long way.
- In current state, a long way away from having these psychologically intelligent models.
>> And the more you address
those things you put up there, the more trust I think
people are going to have in interacting with them, which
I think is really important.
So let's talk a little bit
about how you kind of go about making this happen, right?
>> Yeah.
- Receptiviti and just in general.
And again, let me bring up
another graphic here for you to kind of explain.
>> Yeah. So let me, before step one here, what
we have is we've got a system that's been built on just
a massive quantity of psycho-linguistics,
psychological and linguistic and research with just, again, a huge number of research citations.
And it's been used across so
many different domains to try and understand or to
understand the relationship between language and psychology and language of cognitive processes.
And so when language is analyzed, we're

[30:47]
using algorithms that capture
these subtle signals based on how people use prepositions,
articles, pronouns, different types of function words, and those algorithms
convert that into vectors with psychological metadata.
And so basically will understand, as you can see on the
left side of this chart, if you throw language into the system, say a prompt history from an individual, and we use that as an example here for a large language model,
if I were to take, Scott, your prompt history from a
large language model, feed it through our system, you'd
get back about 200 different dimensions that reflect different aspects of your psychology, cognitive
processing and whatnot.
And so this is just one example here.
We've taken four of the more dominant traits
from an individual whose historical language was analyzed, and what we found was
most dominant here is low agreeableness, high
impulsiveness, high neuroticism, and this kind of low
analytical thinking style.
And so if you feed those
into a large language model,

[31:49]
there's kind of two ways you can do it.
You could feed it into a large
language model, inject it as part of a prompt and
say, "When you respond to this individual,
based on their history, they are low on agreeableness,
high on impulsiveness, high on neuroticism, and
a low analytical thinker.
Tailor your response to those traits, those features," and the large language model will do a relatively good job at it.
But you can actually improve
it even more if you say to the large language model first, "Based on these four
attributes, agreeableness, impulsiveness, neuroticism, and analytical thinking, how would you refine a response so that it would be best
received by that individual?
" And so you take then the
instructions it generates and feed those into the prompt and it responds in a way
that reflects the way that person thinks and feels.
>> Right. You provided-
- Sorry, go ahead.
>> I was going to say you provided a good example of that here.
>> Yeah.
- The left and right.

[32:51]
>> Exactly.
- Is it some right?
>> Yeah, so if you take
this exact example here, the generic agent behavior
on the left is just this kind of generic response.
There's not much to it.
It's just kind of wrote, "Here are the instructions I was told to follow, and here's what we got.
" If you're anxious and neurotic, this response may not
necessarily sit with you just right at the end of the day.
If you're impulsive and in a hurry, this
response may not necessarily jive with you very well.
So you take a look at
what happens on the right.
You inject those four
recommendations into either these four psychological vectors or four psychological signals or the four communication recommendations, and you get a response
back that better jives with the person that you saw
on the previous slide, right?

[33:53]
You've got the large language model right out of the gate saying, "No problem.
I can help you reset
the password right now.
" So that calms the person
down a little bit, right?
This is somebody who is showing
some signs of neuroticism.
Also addressing them and saying, "We're going
to get this sorted quickly.
I need to confirm a few details
to protect your account.
" And why it said, "Protect
your account," as opposed to just, "I need to
confirm a few details," is because it's calming this
person down a little bit and saying, "I'm asking you information for your own security.
" And so on and so forth.
And at the end, it also says,
"Just a couple of quick steps.
We're not going to keep you waiting and I'll stay with you the whole way.
" So this is somebody who's, again, somebody who's a little bit neurotic and it's somebody who's
low in analytical thinking and it's really kind of walking them through the process with kid gloves on.
>> Yeah, you can clearly see
the difference well, "I need to verify your identity,"
versus, "I just want to help you make sure that you're protected," has a different tone.
>> Yeah.
- And this is executing a task which is

[34:55]
resetting your password.
But again, when you get into
this, you're trying to work with these things to
plan and make decisions.
I can see where this becomes really...
>> But that password reset
example is an interesting one because when you do that, you're in a different psychological
state than you probably were five minutes before when you didn't need to reset the password.
Suddenly, especially in this
day and age where cybersecurity and people are worried about
getting hacked, as soon as I engage with something
about resetting a password, I'm kind of on the defensive and I'm kind of worried a little bit and looking out for my best interest and a little concerned, "Is
this a legitimate activity?
Have I been thrown into some
sort of workflow by some sort of nefarious actor and am I
potentially compromising my security by giving them
too much information?
" So having a response
that really does address me as the person I am and acknowledges where I'm at can be immensely helpful at the end of the day in building the trust you were talking about earlier.
>> Yeah. So when you guys, Receptiviti, you mentioned since 2015

[35:56]
or so you guys have been doing this.
Now, we're in this world of AI.
And so how are you
expanding what you're doing?
Is it basically you're
creating the APIs that an agent or can interact with an LLM and your system and bring it all together?
Is that sort of how it works?
>> So it's interesting.
I'll go a little beyond
the question you're asking.
When we started this, we
actually launched an API that was doing personality
insights slightly before IBM Watson launched theirs.
We've always been a little bit ahead, and that wasn't by purpose, it was just by happenstance at the end of the day.
The launch and the proliferation
of large language models has been really, really interesting for us because when large
language models emerged, we looked at them with a little
bit of trepidation saying,

[36:57]
"Ooh, these are really interesting.
How quickly are they going
to be able to do what we do?
" And what we've found is
that they can't do what we do.
And so large language models, they haven't changed our
business dramatically, because at the end of the
day, we're in the business of helping organizations
better understand the people that are important to them.
And so these agentic AI companies, these large language model companies, and the companies that are building off of large language models,
they have the same problem, the same concern that our much earlier customers had at
the end of the day, which is who are these people
we're interacting with?
And the beautiful thing is, is that large language model companies and all of this technology
we're talking about today, it's all centered around language.
So we've got so much of the
raw data at our disposal that we can convert into signal.
It hasn't changed our business, it's just broadened the
applications for our business.
I think the other way that it's
changing things too is a lot

[37:59]
of the analysis that our customers
used to do was less time- sensitive at the end of the day.
"Help me analyze this audience so that I can better understand
who they are so I can go and cluster this audience and put up marketing messages
that are going to resonate with them at a psychological level.
" Great example of the sort
of work that we've done.
And what's different today is kind of the speed at which these
sort of insights are going to be required because
dealing with real-time systems with large language models.
So those present really interesting
opportunities to build a persona of who somebody is and then continuously
update it based on the language that they're generating.
So now I understand this is
what Scott is typically like, and in this conversation,
Scott is a little happier or a little less stressed, and I can treat him differently based on that.
So it's the dynamic nature
that's changed, the immediacy of the immediacy of the need
at the end of the day and feeding these into live
systems has become much easier.
So at the end of the day, we
are not changing the foundation

[39:01]
of our business in any way.
It's really about how this is applied and where it's applied.
>> Yeah.
- In many ways, I think we're more relevant
today than we've ever been.
>> Yeah, no, again, I think as you go, if I step back and just
think about, this is podcast number 17 that I've done.
So I've talked to a whole bunch
of pioneers like yourself, and if I bring this all together as we're moving from a
world of generative AI, what we would call
assistance, AI assistance is what we call it here at theCUBE Research, which are task-oriented.
They can help automate
repetitive tasks, simple tasks.
You can create documents and content.
You can analyze information.
It's like search on steroids,
all that kind of stuff, right?
Agents are more about helping
you accomplish a goal.
>> Yeah.
- Decisions, problem solve.
They become more sophisticated, and you're starting to make
real business decisions.
And that trust factor goes up.

[40:01]
And what I've sort of pulled together, and let me just show you a chart on this to get your feedback on
it, is it's clear to me that LLMs are the foundation, but for agentic AI to realize
its promise, particularly as we move forward in time
from here, there's going to be the need for these
extended models, if you will, that plug into and build around the LLM.
And they're all based in
mathematical or science, right?
Semantic knowledge,
mirror knowledge, graphs.
I can talk to the causal
reasoning, which is the dual calc that Judea Pearl talked about, about the dynamic nature
of cause and effect.
Swarm intelligence has
its own mathematical...
I can't even say it.
There's math behind all that, the algorithms.
And I think what you're
saying is there's also science and math behind the
psychological intelligence, and they can all be built
into these extended models and APIs that then could
be all integrated together.
And the whole damn system learns
from each other throughout

[41:02]
all these facets.
And so this is sort of like
a ladder of new capabilities that are very likely to come
on the scene in the context of agents as we go forward.
So what do you think about this chart?
>> Yeah, I think you're absolutely right.
I'm going to butcher his
name, Yann LeCun, at the end of day has talked about this idea that artificial intelligence needs world models at the end of the day.
And I think this aligns quite well.
And what you were showing
just now aligns really, really well with that
at the end of the day.
We have built these systems that are intended to serve humanity.
They're intended to work with us.
They're intended, as you said, they're assistants at the end of the day.
And assistants can only be so good if they don't really understand who they're servicing,
who they're serving.
And so for these systems truly
to become partners, they need to have, maybe not true empathy, but they need to interact with us in ways

[42:03]
that feel more empathetic, that go beyond just the simple kind
of transactional interactions to ones that feel more
human at the end of the day.
We tend to not think about the
social aspect of this, right?
Talking to other humans
is a social activity, but we're quickly
stepping into an age here where interacting with
technology is becoming a social experience.
And we're beginning to
expect the same sort of experience from technology as we expect to get from other people.
And so, yeah, absolutely.
I think we're certainly going to be seeing that psychological
intelligence becomes one of these foundational
components of these systems.
They may not be psychologically
intelligent the same way we are, but they'll be able to
pick up on some of those signals and change their behavior,
change what they do in ways that reflect what they
can learn about people.
>> Yeah, I do think the
next frontier, it's going

[43:03]
to be the semantic knowledge.
It's meaning, context and relationships among different things.
Then you get into the psychological stuff that you talked about on this
podcast, then you get into the ability to make decisions
and understand consequences and do what ifs around
the causal AI stuff.
And then you get multiple agents
and models working together and learning from each other
to make even better decisions.
It's just the technology,
that's the next frontier of AI.
All right. Well, Jonathan, I think it's time for us to wrap up.
This has been truly fascinating.
It's an eye-opening conversation.
We could probably talk
all day on this stuff and we'll definitely have you
back sometime in the future and talk even more about this.
So thanks for being here.
>> Pleasure. Thanks for having me, Scott.
>> I think the idea of the
psychologically aware agents is definitely going to reshape how
AI collaborates with humans.
It's a really powerful notion, and as we've been saying, the foundation for advancing the next
wave of digital labor.

[44:06]
So I think this is absolutely fundamental.
So I thank you all for taking
the time to tune in and watch.
I hope this discussion has
sparked some new ideas.
To learn more about all of
this, check out Receptiviti.
They have a bunch of research
and tools on their website, including healthcare
workers and well-being index and things of that nature.
So receptiviti.com, you can
also contact Jonathan or I.
I can help make the contacts
into him if you would like to engage there.
And don't forget, you can
stream all the episodes of the Next Frontier of AI on
demand from YouTube channel, our YouTube channel, and on thecube.
net. We'll see you next time
on the Next Frontiers of AI.
We are the leader in enterprise
SEC news and analysis.
See you all and thanks
again for being here.